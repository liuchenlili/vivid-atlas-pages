---
title: 📉 数据降维方法全解析
summary: 一文掌握 PCA、LDA、t-SNE、UMAP 等主流降维算法的原理、应用与区别。
date: 2025-11-12
authors:
  - admin
tags:
  - 机器学习
  - 数据降维
  - 教学笔记
  - 可视化
math: true
image:
  caption: 'Image credit: [**Unsplash**](https://unsplash.com)'
---

# 数据降维方法详解

## 一、什么是降维

**降维（Dimensionality Reduction）** 是将高维数据映射到低维空间的过程，在尽量保留主要信息的前提下减少特征维度。  
主要目标包括：
- 降低计算复杂度
- 去除冗余特征与噪声
- 便于可视化与理解数据结构

降维方法可分为两类：
- **线性降维**：如 PCA（主成分分析）、LDA（线性判别分析）
- **非线性降维**：如 t-SNE、UMAP

---

## 二、PCA（主成分分析）

### 1. 基本思想
PCA 是一种**无监督的线性降维方法**，通过线性变换将数据投影到新的正交坐标轴上，使得：
- 各轴方向互相独立；
- 在这些方向上数据方差最大。

换句话说，PCA 寻找能解释数据方差最大的方向，从而保留“最有信息量”的部分。

### 2. 数学原理

设原始数据矩阵为 \( X \in \mathbb{R}^{n \times d} \)。

1. **中心化数据**  
   \[
   X_c = X - \bar{X}
   \]

2. **计算协方差矩阵**  
   \[
   \Sigma = \frac{1}{n} X_c^T X_c
   \]

3. **特征值分解**  
   \[
   \Sigma = V \Lambda V^T
   \]
    - \( V \)：特征向量矩阵（主成分方向）
    - \( \Lambda \)：特征值矩阵（方差解释率）

4. **选择前 \( k \) 个主成分**  
   \[
   Z = X_c V_k
   \]
   得到降维后的数据 \( Z \)。

### 3. 方差解释率

\[
\text{解释率}_i = \frac{\lambda_i}{\sum_j \lambda_j}
\]

通常选择累计解释率达到 90%~95% 的前几个主成分。

### 4. 优缺点

**优点：**
- 简单高效，常用于特征压缩
- 去除特征相关性

**缺点：**
- 仅适用于线性结构
- 主成分难以直接解释

---

## 三、LDA（线性判别分析）

### 1. 基本思想
LDA 是一种**有监督的线性降维方法**。  
与 PCA 不同，LDA 利用类别标签信息，通过最大化类间方差、最小化类内方差来寻找最优投影方向。

目标：  
\[
\text{类间方差最大，类内方差最小}
\]

### 2. 数学原理

设数据共有 \( C \) 个类别。

- **类内散度矩阵**：
  \[
  S_W = \sum_{i=1}^{C} \sum_{x \in X_i} (x - \mu_i)(x - \mu_i)^T
  \]
- **类间散度矩阵**：
  \[
  S_B = \sum_{i=1}^{C} n_i (\mu_i - \mu)(\mu_i - \mu)^T
  \]

优化目标：
\[
W^* = \arg\max_W \frac{|W^T S_B W|}{|W^T S_W W|}
\]
通过广义特征值分解求得最优投影矩阵 \( W^* \)。

### 3. 特点
- 降维后最多为 \( C - 1 \) 维
- 常用于分类前特征提取（如人脸识别 Fisherfaces）

---

## 四、t-SNE（t-分布随机邻域嵌入）

### 1. 基本思想
t-SNE 是一种**非线性降维与可视化方法**。  
它通过保持高维空间中样本之间的**局部相似性**，在低维空间中展示高维数据的结构。

### 2. 核心步骤
1. 在高维空间中，用高斯分布计算相似度 \( p_{j|i} \)。
2. 在低维空间中，用 t 分布计算相似度 \( q_{ij} \)。
3. 通过最小化 KL 散度：
   \[
   KL(P||Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
   \]
   使两个分布尽量接近。

### 3. 特点
- 擅长高维数据可视化
- 能揭示聚类结构
- 对参数（如 perplexity）敏感，计算较慢

---

## 五、UMAP（Uniform Manifold Approximation and Projection）

### 1. 基本思想
UMAP 也是一种**非线性降维方法**，基于**流形学习理论**。  
它假设数据分布在一个低维流形上，通过图论建模与优化实现降维。

### 2. 核心流程
1. 在高维空间构建邻接图（保留局部关系）
2. 在低维空间建立对应图
3. 通过优化两图的相似性，保持局部与全局结构

### 3. 优点
- 比 t-SNE 更快、可扩展性更强
- 可保留更多全局结构
- 可用于聚类、嵌入或可视化

---

## 六、方法对比

| 方法 | 类型 | 是否监督 | 保留结构 | 应用场景 | 优点 | 缺点 |
|------|------|-----------|-----------|-----------|------|------|
| PCA | 线性 | 否 | 全局 | 特征压缩、预处理 | 简单高效 | 仅限线性结构 |
| LDA | 线性 | 是 | 类间/类内 | 分类前降维 | 利用标签信息 | 类别数限制 |
| t-SNE | 非线性 | 否 | 局部 | 可视化 | 展示聚类效果佳 | 计算慢 |
| UMAP | 非线性 | 否 | 局部+全局 | 可视化、聚类 | 快速可扩展 | 参数敏感 |

---

## 七、实践建议

- **PCA**：用于预处理和特征压缩；
- **LDA**：用于有监督分类任务前的降维；
- **t-SNE / UMAP**：用于高维数据的可视化与结构探索。

常见实践流程：
1. 数据标准化（StandardScaler）
2. 先用 PCA 降到中等维度
3. 再用 t-SNE 或 UMAP 可视化

---

